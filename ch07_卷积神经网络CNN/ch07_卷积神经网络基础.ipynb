{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 卷积神经网络基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Output\\_size = \\left \\lfloor \\frac{n + 2p - f}{s} + 1\\right \\rfloor \\times \\left \\lfloor \\frac{n + 2p - f}{s} + 1\\right \\rfloor$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1.shape: torch.Size([1, 3, 26, 26])\n",
      "out2.shape: torch.Size([1, 3, 28, 28])\n",
      "out3.shape: torch.Size([1, 4, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Conv2d(1,3,kernel_size=3,stride=1,padding=0)\n",
    "x = torch.rand(1,1,28,28)\n",
    "\n",
    "out = layer.forward(x) # 卷积运算\n",
    "print('out1.shape:',out.shape)\n",
    "\n",
    "layer = nn.Conv2d(1,3,kernel_size=3,stride=1,padding=1)\n",
    "out = layer.forward(x)\n",
    "print('out2.shape:',out.shape)\n",
    "\n",
    "layer = nn.Conv2d(1,4,kernel_size=3,stride=2,padding=1)\n",
    "out = layer.forward(x)\n",
    "print('out3.shape:',out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner weight & bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.1704, -0.0875,  0.0579],\n",
       "          [ 0.2762,  0.2394, -0.0993],\n",
       "          [ 0.1446,  0.0253, -0.1044]]],\n",
       "\n",
       "\n",
       "        [[[-0.1198, -0.3039, -0.3194],\n",
       "          [ 0.2971, -0.2700, -0.0318],\n",
       "          [ 0.1160,  0.1986, -0.3255]]],\n",
       "\n",
       "\n",
       "        [[[-0.1016,  0.0956, -0.0222],\n",
       "          [ 0.1155,  0.1237,  0.1385],\n",
       "          [ 0.2375, -0.2012,  0.3073]]],\n",
       "\n",
       "\n",
       "        [[[-0.3036,  0.2053,  0.2781],\n",
       "          [-0.0271, -0.0822, -0.0675],\n",
       "          [ 0.2384, -0.2601,  0.0878]]]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "x = torch.randn(1,3,28,28) # x 和 w 的 channel 值要相同,均为3\n",
    "w = torch.rand(16,3,5,5)\n",
    "b = torch.rand(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([1, 3, 28, 28])\n",
      "out.shape:  torch.Size([1, 16, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "out = F.conv2d(x,w,b,stride=2,padding=2)\n",
    "print('x.shape: ',x.shape)\n",
    "print('out.shape: ',out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Down/Up Sample 池化层与采样 ReLU层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling\n",
    "\n",
    "### Avg Pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([1, 16, 14, 14])\n",
      "out1.shape:  torch.Size([1, 16, 7, 7])\n",
      "out2.shape:  torch.Size([1, 16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x = out\n",
    "print('x.shape: ',x.shape)\n",
    "\n",
    "layer = nn.MaxPool2d(2,stride=2)\n",
    "\n",
    "out1 = layer(x)\n",
    "print('out1.shape: ',out1.shape)\n",
    "\n",
    "out2 = F.avg_pool2d(x,2,stride=2)\n",
    "print('out2.shape: ',out2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UpSample \n",
    "\n",
    "#### F.interpolate( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([1, 16, 14, 14])\n",
      "out.shape:  torch.Size([1, 16, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x = out\n",
    "print('x.shape: ',x.shape)\n",
    "out = F.interpolate(x,scale_factor=2,mode='nearest')\n",
    "print('out.shape: ',out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([1, 16, 14, 14])\n",
      "layer = nn.ReLU(inplace=True), inplace=True可以节省内存一半空间\n",
      "out.shape:  torch.Size([1, 16, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "print('x.shape: ',x.shape)\n",
    "\n",
    "layer = nn.ReLU(inplace=True)\n",
    "print('layer = nn.ReLU(inplace=True), inplace=True可以节省内存一半空间')\n",
    "out = layer(x)\n",
    "print('out.shape: ',out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape:  torch.Size([1, 16, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "out = F.relu(x)\n",
    "print('out.shape: ',out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.3 Batch Norm\n",
    "\n",
    "### Feature Scaling\n",
    "- Image Normalization:\n",
    "```Python\n",
    "normaliza = transforms.Normaliza(mean=[0.485,0.456,0.406],\n",
    "                                 std=[0.229,0.224,0.225])\n",
    "# RGB 各通道的均值和方差\n",
    "```\n",
    "- Batch Normalization:\n",
    "\n",
    "$ Algorithm : Batch Normalizing Transform, applied to activation x over a mini-batch.$\n",
    "\n",
    "$Input: Values of x over a mini-batch: \\mathcal{B} = \\{ x_{1...m}\\}; $\n",
    "$Output: \\{ y_i = BN_{\\gamma , \\beta}(x_i)\\} $\n",
    "\n",
    "$$mini-batch \\space mean:  \\mu_{\\mathcal{B}} \\leftarrow \\frac{1}{m}\\sum_{i=1}^m x_i $$\n",
    "\n",
    "$$mini-batch \\space variance: \\sigma_{\\mathcal{B}}^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^m (x_i-\\mu_\\mathcal{B})^2 $$\n",
    "\n",
    "$$normalize: \\hat{x_i} \\leftarrow \\frac{x_i - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_\\mathcal{B}^2 + \\varepsilon }}$$\n",
    "\n",
    "$$scale \\space and \\space shift: y_i \\leftarrow \\gamma \\hat{x_i} + \\beta \\equiv BN_{\\gamma , \\beta (x_i)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([100, 16, 784])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(100,16,784) # 784 = 28*28\n",
    "print('x.shape: ',x.shape)\n",
    "layer = nn.BatchNorm1d(16)\n",
    "out = layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.BatchNorm1d(16)的参数16为channel通道数,BatchNormalize通道的统计数据是跨通道运算的,有多少个channel,就计算多少个channel上的均值和方差,16个channel就生成16个长度的统计信息,每个信息代表每个channel的均值和方差**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.running_mean: \n",
      "tensor([-4.5907e-04,  8.6306e-05, -7.4280e-04, -6.2997e-04, -6.4749e-04,\n",
      "         1.4273e-04, -3.9751e-05, -8.0233e-05,  2.4080e-04, -3.0221e-04,\n",
      "         8.5041e-05,  1.8740e-04, -1.6602e-04, -6.1695e-05, -3.7739e-04,\n",
      "         1.0307e-04])\n",
      "layer.running_var: \n",
      "tensor([1.0003, 0.9996, 1.0000, 1.0000, 1.0006, 1.0003, 1.0000, 1.0004, 1.0000,\n",
      "        0.9994, 0.9999, 0.9996, 0.9997, 1.0008, 0.9995, 0.9999])\n"
     ]
    }
   ],
   "source": [
    "print('layer.running_mean: ')\n",
    "print(layer.running_mean)\n",
    "print('layer.running_var: ')\n",
    "print(layer.running_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.BatchNorm2d\n",
    "\n",
    "layer.weigh $ \\rightarrow \\space \\gamma \\ $\n",
    "\n",
    "layer.bias $ \\rightarrow \\space \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([6, 16, 28, 28])\n",
      "After BatchNorm2d(16), out.shape:  torch.Size([6, 16, 28, 28])\n",
      "layer.weight.shape:  torch.Size([16])\n",
      "layer.bias.shape:  torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(6,16,28,28)\n",
    "print('x.shape: ',x.shape)\n",
    "layer = nn.BatchNorm2d(16)\n",
    "out = layer(x)\n",
    "print('After BatchNorm2d(16), out.shape: ',out.shape)\n",
    "\n",
    "print('layer.weight.shape: ',layer.weight.shape)\n",
    "print('layer.bias.shape: ',layer.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vars()方法打印所有参数:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_backward_hooks': OrderedDict(),\n",
       " '_buffers': OrderedDict([('running_mean',\n",
       "               tensor([0.0501, 0.0498, 0.0502, 0.0494, 0.0505, 0.0503, 0.0507, 0.0497, 0.0508,\n",
       "                       0.0509, 0.0500, 0.0496, 0.0496, 0.0510, 0.0501, 0.0502])),\n",
       "              ('running_var',\n",
       "               tensor([0.9084, 0.9082, 0.9082, 0.9082, 0.9084, 0.9082, 0.9084, 0.9083, 0.9083,\n",
       "                       0.9084, 0.9082, 0.9083, 0.9085, 0.9083, 0.9083, 0.9084])),\n",
       "              ('num_batches_tracked', tensor(1))]),\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " '_parameters': OrderedDict([('weight', Parameter containing:\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "                      requires_grad=True)), ('bias', Parameter containing:\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      requires_grad=True))]),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " 'affine': True,\n",
       " 'eps': 1e-05,\n",
       " 'momentum': 0.1,\n",
       " 'num_features': 16,\n",
       " 'track_running_stats': True,\n",
       " 'training': True}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('vars()方法打印所有参数:')\n",
    "vars(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 时注意事项:\n",
    "test时,均值和方差取的不是当前值,而是全局值(running),$\\gamma$和$\\beta$不需要backward,即不需要更新只需切换test模式,使用 **layer.eval()**\n",
    "\n",
    "```Python\n",
    "layer.eval()\n",
    "BatchNorm1d(16,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)\n",
    "out = layer(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python35664bitbaseconda34414b764a4544e4b3502fc9f239efc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
