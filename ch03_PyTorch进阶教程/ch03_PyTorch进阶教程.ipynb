{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch03_PyTorch进阶教程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Broadcasting 广播 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,3)\n",
    "print(a)\n",
    "b = a+5\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note1: Broadcasting\n",
    "- situation 1:\n",
    " - tensorA : [4,32,14,14]\n",
    " - tensorB : [1,32,1,1] -> [4,32,14,14]\n",
    "- situation 2:\n",
    " - tensorA : [4,32,14,14]\n",
    " - tensorB : [14,14] -> [1,1,14,14] -> [4,32,14,14]\n",
    "- situation 3:\n",
    " - tenosrA : [4,32,14,14]\n",
    " - tensorB : [2,32,14,14] -> 手动完成 + B[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: 拼接与拆分\n",
    "- cat\n",
    "- stack\n",
    "- split\n",
    "- chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat函数,不创建新的维度:\n",
      "a1:  torch.Size([4, 3, 32, 32]) \n",
      "a2:  torch.Size([4, 3, 32, 32])\n",
      "torch.cat([a1,a2],dim=0):  torch.Size([8, 3, 32, 32]) \n",
      "\n",
      "stack函数,会创建新的维度:\n",
      "a1:  torch.Size([4, 3, 32, 32]) \n",
      "a2:  torch.Size([4, 3, 32, 32])\n",
      "torch.stack([a1,a2],dim=2):  torch.Size([2, 4, 3, 32, 32]) \n",
      "\n",
      "splic函数,根据长度拆分:\n",
      "c:  torch.Size([3, 32, 8])\n",
      "c1,c2 = c.split([2,1],dim=0):  c1: torch.Size([2, 32, 8])   c2: torch.Size([1, 32, 8])\n",
      "上面这种写法,给split传入一个列表,[2,1]表示在第0维分成两个tenor,一个长为2一个长为1\n",
      "\n",
      "d:  torch.Size([6, 32, 8])\n",
      "d1,d2 = d.split(3,dim=0):  d1: torch.Size([3, 32, 8])   d2: torch.Size([3, 32, 8])\n",
      "上面这种写法,3表示长度,在第0维将d分成6/3=2个tensor,每个tensor长为3\n",
      "\n",
      "chunk函数,按数量拆分:\n",
      "a1:  torch.Size([4, 32, 8]) \n",
      "a2:  torch.Size([4, 32, 8])\n",
      "a1,a2=a.chunk(2,dim=0):  a1:  torch.Size([4, 32, 8])   a2: torch.Size([4, 32, 8])\n"
     ]
    }
   ],
   "source": [
    "# cat\n",
    "print('cat函数,不创建新的维度:')\n",
    "a1 = torch.rand(4,3,32,32)\n",
    "a2 = torch.rand(4,3,32,32)\n",
    "print('a1: ',a1.shape,'\\na2: ',a2.shape)\n",
    "print('torch.cat([a1,a2],dim=0): ',torch.cat([a1,a2],dim=0).shape,'\\n')\n",
    "\n",
    "print('stack函数,会创建新的维度:')\n",
    "print('a1: ',a1.shape,'\\na2: ',a2.shape)\n",
    "print('torch.stack([a1,a2],dim=2): ',torch.stack([a1,a2],dim=0).shape,'\\n')\n",
    "\n",
    "print('splic函数,根据长度拆分:')\n",
    "c = torch.rand(3,32,8)\n",
    "print('c: ',c.shape)\n",
    "c1,c2 = c.split([2,1],dim=0)\n",
    "print('c1,c2 = c.split([2,1],dim=0): ','c1:',c1.shape,'  c2:',c2.shape)\n",
    "print('上面这种写法,给split传入一个列表,[2,1]表示在第0维分成两个tenor,一个长为2一个长为1\\n')\n",
    "\n",
    "\n",
    "d = torch.rand(6,32,8)\n",
    "print('d: ',d.shape)\n",
    "d1,d2 = d.split(3,dim=0)\n",
    "print('d1,d2 = d.split(3,dim=0): ','d1:',d1.shape,'  d2:',d2.shape)\n",
    "print('上面这种写法,3表示长度,在第0维将d分成6/3=2个tensor,每个tensor长为3\\n')\n",
    "\n",
    "print('chunk函数,按数量拆分:')\n",
    "a = torch.rand(8,32,8)\n",
    "a1,a2 = a.chunk(2,dim=0)\n",
    "print('a1: ',a1.shape,'\\na2: ',a2.shape)\n",
    "print('a1,a2=a.chunk(2,dim=0): ','a1: ',a1.shape,'  a2:',a2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 数学运算\n",
    "- add/sub/mul/div   +-*/\n",
    "- matmul 矩阵相乘\n",
    " - torch.mm (only for 2d)\n",
    " - torch.matmul (推荐)\n",
    " - @ (numpy)\n",
    "- pow sqrt/rsqrt\n",
    "- exp log log2 log10\n",
    "- round\n",
    " - .floor()  .ceil()\n",
    " - .round()\n",
    " - .trunc()  .frac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul矩阵乘法:\n",
      "a.shape:  torch.Size([4, 3, 28, 64])   b.shape:  torch.Size([4, 3, 64, 32])\n",
      "对于2D以上的tensor乘法,只能用matmul,不能用mm,matmul只取最后两维:\n",
      "torch.matmul(a,b) 即[28,64]*[64,32]:  torch.Size([4, 3, 28, 32]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('matmul矩阵乘法:')\n",
    "a = torch.rand(4,3,28,64)\n",
    "b = torch.rand(4,3,64,32)\n",
    "print('a.shape: ',a.shape,'  b.shape: ',b.shape)\n",
    "print('对于2D以上的tensor乘法,只能用matmul,不能用mm,matmul只取最后两维:')\n",
    "print('torch.matmul(a,b) 即[28,64]*[64,32]: ',torch.matmul(a,b).shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power用法:\n",
      "a:\n",
      " tensor([[16., 16.],\n",
      "        [16., 16.]])\n",
      "a**2:\n",
      " tensor([[256., 256.],\n",
      "        [256., 256.]])\n",
      "a.pow(2):\n",
      " tensor([[256., 256.],\n",
      "        [256., 256.]])\n",
      "a.sqrt():\n",
      " tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "a.rsqrt() 平方根的倒数:\n",
      " tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "print('power用法:')\n",
    "a = torch.full([2,2],16)\n",
    "print('a:\\n',a)\n",
    "print('a**2:\\n',a**2)\n",
    "print('a.pow(2):\\n',a.pow(2))\n",
    "print('a.sqrt():\\n',a.sqrt())\n",
    "print('a.rsqrt() 平方根的倒数:\\n',a.rsqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp log 用法:\n",
      "a = torch.exp(torch.ones(2,2)):\n",
      " tensor([[2.7183, 2.7183],\n",
      "        [2.7183, 2.7183]])\n",
      "torch.log(a):\n",
      " tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]])\n",
      "torch.log10(a):\n",
      " tensor([[0.4343, 0.4343],\n",
      "        [0.4343, 0.4343]])\n",
      "torch.log2(a):\n",
      " tensor([[1.4427, 1.4427],\n",
      "        [1.4427, 1.4427]])\n"
     ]
    }
   ],
   "source": [
    "print('exp log 用法:')\n",
    "a = torch.exp(torch.ones(2,2))\n",
    "print('a = torch.exp(torch.ones(2,2)):\\n',a)\n",
    "print('torch.log(a):\\n',torch.log(a))\n",
    "print('torch.log10(a):\\n',torch.log10(a))\n",
    "print('torch.log2(a):\\n',torch.log2(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floor() 和 ceil() 用法:\n",
      "a:  tensor(3.1400)\n",
      "a.floor()向上取整:  tensor(3.)\n",
      "a.ceil()向下取整:  tensor(4.)\n",
      "\n",
      "\n",
      "trunc和frac将小数裁剪,trunc取整数部分,frac取小数部分:\n",
      "a.trunc():  tensor(3.)\n",
      "a.frac():  tensor(0.1400)\n",
      "\n",
      "\n",
      "round四舍五入:\n",
      "a.round():  tensor(3.)\n",
      "\n",
      "\n",
      "clamp多用于梯度裁剪:\n",
      "grad: \n",
      " tensor([[ 5.7228,  0.5189,  1.9334],\n",
      "        [14.1330,  3.7156, 11.3122]])\n",
      "grad.max():  tensor(14.1330)\n",
      "grad.median() 中间值:  tensor(3.7156) \n",
      "\n",
      "grad.clamp(10),小于10的都赋值为10: \n",
      " tensor([[10.0000, 10.0000, 10.0000],\n",
      "        [14.1330, 10.0000, 11.3122]]) \n",
      "\n",
      "grad.clamp(5,10),小于5的赋值为5,大于是10的赋值为10: \n",
      " tensor([[ 5.7228,  5.0000,  5.0000],\n",
      "        [10.0000,  5.0000, 10.0000]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('floor() 和 ceil() 用法:')\n",
    "a = torch.tensor(3.14)\n",
    "print('a: ',a)\n",
    "print('a.floor()向上取整: ',a.floor())\n",
    "print('a.ceil()向下取整: ',a.ceil())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('trunc和frac将小数裁剪,trunc取整数部分,frac取小数部分:')\n",
    "print('a.trunc(): ',a.trunc())\n",
    "print('a.frac(): ',a.frac())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('round四舍五入:')\n",
    "print('a.round(): ',a.round())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('clamp多用于梯度裁剪:')\n",
    "grad = torch.rand(2,3)*15\n",
    "print('grad: \\n',grad)\n",
    "print('grad.max(): ',grad.max())\n",
    "print('grad.median() 中间值: ',grad.median(),'\\n')\n",
    "print('grad.clamp(10),小于10的都赋值为10: \\n',grad.clamp(10),'\\n')\n",
    "print('grad.clamp(5,10),小于5的赋值为5,大于是10的赋值为10: \\n',grad.clamp(5,10),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 属性统计\n",
    "- norm\n",
    "- mean sum\n",
    "- prod\n",
    "- max,min,argmin,argmax\n",
    "- kthvalue,topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm 范数:\n",
      "a:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "a的1范数为所有元素的绝对值之和 a.norm(1):  tensor(8.)\n",
      "a的2范数为所有元素绝对值的平方和开根号 a.norm(2):  tensor(2.8284)\n",
      "\n",
      "b:\n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "b.norm(1,dim=1) b在第一维上的1范数:  tensor([4., 4.])\n",
      "\n",
      "c: tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]]])\n",
      "c.norm(2,dim=0) c在第0维上的2范数:  tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "print('norm 范数:')\n",
    "a = torch.full([8],1)\n",
    "b = a.view(2,4)\n",
    "c = a.view(2,2,2)\n",
    "print('a:\\n',a)\n",
    "print('a的1范数为所有元素的绝对值之和 a.norm(1): ',a.norm(1))\n",
    "print('a的2范数为所有元素绝对值的平方和开根号 a.norm(2): ',a.norm(2))\n",
    "\n",
    "print('\\nb:\\n',b)\n",
    "print('b.norm(1,dim=1) b在第一维上的1范数: ',b.norm(1,dim=1))\n",
    "\n",
    "print('\\nc:',c)\n",
    "print('c.norm(2,dim=0) c在第0维上的2范数: ',c.norm(2,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]])\n",
      "a.min():  tensor(0.)\n",
      "a.max():  tensor(7.)\n",
      "a.sum():  tensor(28.)\n",
      "a.mean():  tensor(3.5000)\n",
      "a.prod()累乘:  tensor(0.)\n",
      "a.argmax() flatten后最大值的索引:  tensor(7)\n",
      "a.argmin() flatten后最小值的索引:  tensor(0)\n",
      "a.argmax(dim=1) 第一维度:  tensor([1, 1, 1, 1])\n",
      "\n",
      "\n",
      "a:\n",
      " tensor([[0.2580, 0.2123, 0.3272, 0.8038, 0.2387, 0.2304, 0.3114, 0.7600, 0.7949,\n",
      "         0.4442],\n",
      "        [0.9829, 0.9999, 0.4356, 0.5968, 0.3660, 0.2467, 0.7214, 0.9781, 0.3954,\n",
      "         0.3355],\n",
      "        [0.3723, 0.3252, 0.3333, 0.2286, 0.2606, 0.6997, 0.1471, 0.9226, 0.4864,\n",
      "         0.8233],\n",
      "        [0.1116, 0.4671, 0.5081, 0.5448, 0.2634, 0.3047, 0.4667, 0.9692, 0.4291,\n",
      "         0.0211]]) \n",
      "\n",
      "a.max(dim=1): \n",
      " torch.return_types.max(\n",
      "values=tensor([0.8038, 0.9999, 0.9226, 0.9692]),\n",
      "indices=tensor([3, 1, 7, 7])) \n",
      "\n",
      "a.argmax(dim=1: ) tensor([3, 1, 7, 7]) \n",
      "\n",
      "a.max(dim=1,keepdim=True): \n",
      " torch.return_types.max(\n",
      "values=tensor([[0.8038],\n",
      "        [0.9999],\n",
      "        [0.9226],\n",
      "        [0.9692]]),\n",
      "indices=tensor([[3],\n",
      "        [1],\n",
      "        [7],\n",
      "        [7]])) \n",
      "\n",
      "a.argmax(dim=1,keepdim=True):  tensor([[3],\n",
      "        [1],\n",
      "        [7],\n",
      "        [7]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(8).view(2,4).float()\n",
    "print('a: ',a)\n",
    "print('a.min(): ',a.min())\n",
    "print('a.max(): ',a.max())\n",
    "print('a.sum(): ',a.sum())\n",
    "print('a.mean(): ',a.mean())\n",
    "print('a.prod()累乘: ',a.prod())\n",
    "print('a.argmax() flatten后最大值的索引: ',a.argmax())\n",
    "print('a.argmin() flatten后最小值的索引: ',a.argmin())\n",
    "print('a.argmax(dim=1) 第一维度: ',a.argmax(dim=0))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "a = torch.rand(4,10)\n",
    "print('a:\\n',a,'\\n')\n",
    "print('a.max(dim=1): \\n',a.max(dim=1),'\\n')\n",
    "print('a.argmax(dim=1: )',a.argmax(dim=1),'\\n')\n",
    "\n",
    "print('a.max(dim=1,keepdim=True): \\n',a.max(dim=1,keepdim=True),'\\n')\n",
    "print('a.argmax(dim=1,keepdim=True): ',a.argmax(dim=1,keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk or kthvalue用法:\n",
      "a:\n",
      " tensor([[0.6375, 0.0859, 0.2007, 0.9008, 0.3690, 0.4261],\n",
      "        [0.0794, 0.5686, 0.8968, 0.2543, 0.5664, 0.6118]]) \n",
      "\n",
      "a.topk(3,dim=1)在第一维上取概率前3大的元素:\n",
      " torch.return_types.topk(\n",
      "values=tensor([[0.9008, 0.6375, 0.4261],\n",
      "        [0.8968, 0.6118, 0.5686]]),\n",
      "indices=tensor([[3, 0, 5],\n",
      "        [2, 5, 1]])) \n",
      "\n",
      "a.topk(3,dim=1,largest=False)在第一维取最小的3个元素:\n",
      " torch.return_types.topk(\n",
      "values=tensor([[0.0859, 0.2007, 0.3690],\n",
      "        [0.0794, 0.2543, 0.5664]]),\n",
      "indices=tensor([[1, 2, 4],\n",
      "        [0, 3, 4]])) \n",
      "\n",
      "a.kthvalue(6,dim=1)在第一维上取第6小的元素,即第1大,默认是从小到大排序:\n",
      " torch.return_types.kthvalue(\n",
      "values=tensor([0.9008, 0.8968]),\n",
      "indices=tensor([3, 2])) \n",
      "\n",
      "a.kthvalue(6,dim=1,keepdim=True)在第一维上取第6小的元素,即第1大,默认是从小到大排序:\n",
      " torch.return_types.kthvalue(\n",
      "values=tensor([[0.9008],\n",
      "        [0.8968]]),\n",
      "indices=tensor([[3],\n",
      "        [2]])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('topk or kthvalue用法:')\n",
    "a = torch.rand(2,6)\n",
    "print('a:\\n',a,'\\n')\n",
    "print('a.topk(3,dim=1)在第一维上取概率前3大的元素:\\n',a.topk(3,dim=1),'\\n')\n",
    "\n",
    "print('a.topk(3,dim=1,largest=False)在第一维取最小的3个元素:\\n',a.topk(3,dim=1,largest=False),'\\n')\n",
    "\n",
    "print('a.kthvalue(6,dim=1)在第一维上取第6小的元素,即第1大,默认是从小到大排序:\\n',\n",
    "     a.kthvalue(6,dim=1),'\\n')\n",
    "\n",
    "print('a.kthvalue(6,dim=1,keepdim=True)在第一维上取第6小的元素,即第1大,默认是从小到大排序:\\n',\n",
    "     a.kthvalue(6,dim=1,keepdim=True),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "比较函数:\n",
      "a:\n",
      " tensor([[0.2238, 0.6011, 0.9244],\n",
      "        [0.0446, 0.0427, 0.3566],\n",
      "        [0.9677, 0.6089, 0.1657]])\n",
      "a>0:\n",
      " tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "torch.gt(a,0):\n",
      " tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "\n",
      "\n",
      "\n",
      "a:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b:\n",
      " tensor([[0.4793, 0.5230, 0.0222],\n",
      "        [0.3620, 0.8653, 0.4292]])\n",
      "torch.eq(a,b):  tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "torch.eq(a,a):  tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "torch.equal(a,a):  True\n"
     ]
    }
   ],
   "source": [
    "print('比较函数:')\n",
    "a = torch.rand(3,3)\n",
    "print('a:\\n',a)\n",
    "\n",
    "print('a>0:\\n',a>0)\n",
    "print('torch.gt(a,0):\\n',torch.gt(a,0))\n",
    "print('\\n\\n')\n",
    "\n",
    "a = torch.ones(2,3)\n",
    "b = torch.rand_like(a)\n",
    "print('a:\\n',a)\n",
    "print('b:\\n',b)\n",
    "print('torch.eq(a,b): ',torch.eq(a,b))\n",
    "print('torch.eq(a,a): ',torch.eq(a,a))\n",
    "print('torch.equal(a,a): ',torch.equal(a,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 高阶操作\n",
    "- where\n",
    "- gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where : torch.where(condition,x,y) -> Tensor \n",
      "\n",
      "cond:\n",
      " tensor([[0.6000, 0.7000],\n",
      "        [0.8000, 0.4000]])\n",
      "a:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "b:\n",
      " tensor([[100., 100.],\n",
      "        [100., 100.]])\n",
      "\n",
      " torch.where(cond>0.5,a,b):\n",
      " tensor([[  0.,   0.],\n",
      "        [  0., 100.]])\n"
     ]
    }
   ],
   "source": [
    "print('where : torch.where(condition,x,y) -> Tensor \\n')\n",
    "\n",
    "cond = torch.tensor([[0.6,0.7],[0.8,0.4]])\n",
    "a = torch.zeros(2,2)\n",
    "b = torch.ones(2,2)*100\n",
    "print('cond:\\n',cond)\n",
    "print('a:\\n',a)\n",
    "print('b:\\n',b)\n",
    "print('\\n torch.where(cond>0.5,a,b):\\n',torch.where(cond>0.5,a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather : torch.gather(input,dim,index,out=None) -> Tensor\n",
      "prob:\n",
      " tensor([[0.5026, 0.8827, 0.5593, 0.0066, 0.9751, 0.6705, 0.3195, 0.8601, 0.5605,\n",
      "         0.1815],\n",
      "        [0.7326, 0.2979, 0.8574, 0.7390, 0.2142, 0.2609, 0.8189, 0.1418, 0.9417,\n",
      "         0.9883],\n",
      "        [0.5461, 0.6960, 0.3400, 0.4376, 0.1514, 0.4703, 0.3769, 0.6260, 0.9581,\n",
      "         0.5978],\n",
      "        [0.4863, 0.7357, 0.8359, 0.5174, 0.3940, 0.6628, 0.4596, 0.8769, 0.4706,\n",
      "         0.0394]]) \n",
      "\n",
      "idx:\n",
      " torch.return_types.topk(\n",
      "values=tensor([[0.9751, 0.8827, 0.8601],\n",
      "        [0.9883, 0.9417, 0.8574],\n",
      "        [0.9581, 0.6960, 0.6260],\n",
      "        [0.8769, 0.8359, 0.7357]]),\n",
      "indices=tensor([[4, 1, 7],\n",
      "        [9, 8, 2],\n",
      "        [8, 1, 7],\n",
      "        [7, 2, 1]])) \n",
      "\n",
      "idx=idx[1]:\n",
      " tensor([[4, 1, 7],\n",
      "        [9, 8, 2],\n",
      "        [8, 1, 7],\n",
      "        [7, 2, 1]]) \n",
      "\n",
      "label:\n",
      " tensor([100, 101, 102, 103, 104, 105, 106, 107, 108, 109]) \n",
      "\n",
      "label.expand(4,10):\n",
      " tensor([[100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
      "        [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
      "        [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
      "        [100, 101, 102, 103, 104, 105, 106, 107, 108, 109]]) \n",
      "\n",
      "torch.gather(label.expand(4,10),dim=1,index=idx.long()):\n",
      " tensor([[104, 101, 107],\n",
      "        [109, 108, 102],\n",
      "        [108, 101, 107],\n",
      "        [107, 102, 101]])\n"
     ]
    }
   ],
   "source": [
    "print('gather : torch.gather(input,dim,index,out=None) -> Tensor')\n",
    "prob = torch.rand(4,10)\n",
    "idx = prob.topk(dim=1,k=3)\n",
    "print('prob:\\n',prob,'\\n')\n",
    "print('idx:\\n',idx,'\\n')\n",
    "idx = idx[1]\n",
    "print('idx=idx[1]:\\n',idx,'\\n')\n",
    "\n",
    "label = torch.arange(10)+100\n",
    "print('label:\\n',label,'\\n')\n",
    "label = label.expand(4,10)\n",
    "print('label.expand(4,10):\\n',label,'\\n')\n",
    "\n",
    "g = torch.gather(label,dim=1,index=idx.long())\n",
    "print('torch.gather(label.expand(4,10),dim=1,index=idx.long()):\\n',g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "gather查表,idx中第一行为[4,1,7],在label中,第四个元素为104,第一个元素为101,第七个元素是107,所以gather得到的第一行是[104,101,107],以此类推"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python35664bitbaseconda34414b764a4544e4b3502fc9f239efc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
