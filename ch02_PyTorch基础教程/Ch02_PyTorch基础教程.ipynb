{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch02 PyTorch基础教程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 张量数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a.type())\n",
    "print(type(a))\n",
    "isinstance(a,torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note1: 类型推断\n",
    "- tensor_name.type()\n",
    "- isinstance(torsor_name,type_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) torch.Size([]) 0 0\n"
     ]
    }
   ],
   "source": [
    "# 标量 ,dim=0\n",
    "a = torch.tensor(1.)\n",
    "print(a,a.shape,len(a.shape),a.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note2: dim=0 标量\n",
    "- a.shape 成员\n",
    "- a.size() 成员函数\n",
    "- a.dim() 维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1000, 2.2000]) torch.Size([2]) torch.FloatTensor\n",
      "tensor([8.4078e-45, 0.0000e+00]) torch.Size([2]) torch.FloatTensor\n",
      "tensor([1., 1.], dtype=torch.float64) torch.Size([2]) torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "# 向量,dim=1\n",
    "a = torch.tensor([1.1,2.2])\n",
    "print(a,a.shape,a.type())\n",
    "\n",
    "b = torch.FloatTensor(2)\n",
    "print(b,b.shape,b.type())\n",
    "\n",
    "data = np.ones(2)\n",
    "c = torch.from_numpy(data)\n",
    "print(c,c.shape,c.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note3: dim=1 向量\n",
    "- .tensor接受的是数据的内容\n",
    "- .FloatTensor接受的是数据的shape,随机初始化\n",
    "- .from_numpy()  numpy -> torch.tensor\n",
    "- dim=0 -> 0.1\n",
    "- dim=1 -> [0.1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6025, -0.0328, -1.7973],\n",
      "        [ 0.5828,  0.6371, -0.0266]])\n",
      "torch.Size([2, 3])   2   3\n",
      "用shape索引:  2   3\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3) # normal distribution 正态分布\n",
    "# a = torch.FloatTensor(2,3)\n",
    "print(a)\n",
    "print(a.shape,' ',a.size(0),' ',a.size(1))\n",
    "print('用shape索引: ',a.shape[0],' ',a.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9681, 0.9093, 0.3873],\n",
       "         [0.2986, 0.7554, 0.5381]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])   torch.Size([1, 2, 3])\n",
      "a的内存大小为:1x2x3 =  6\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1,2,3)   #  均匀分布[0,1] uniform distribution\n",
    "display(a)\n",
    "print(a.shape,' ',a.size())\n",
    "list(a.shape)\n",
    "print('a的内存大小为:1x2x3 = ',a.numel())\n",
    "print(len(a.shape),a.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note4: dim=3的张量\n",
    "- rand 均匀分布[0,1]\n",
    "- randn 正态分布[0,1]\n",
    "- 为了和python交互,使用list将shape转换为list类型\n",
    "- 用.numel()方法得到tensor的大小\n",
    "- 用len(a.shape)和a.dim()都可得到tensor的维度\n",
    "- 三维张量在RNN经常使用,四维张量在CNN中经常使用,用来表示图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 创建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 3.3000], dtype=torch.float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_np = np.array([2,3.3])\n",
    "a_pt = torch.from_numpy(a_np)\n",
    "a_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 3.2000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([2.,3.2]) # 小写给具体数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.8863e+36, 4.5790e-41, 9.8866e+36],\n",
       "        [4.5790e-41, 2.7604e+20, 1.7744e+28]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(2,3) # 大写给shape\n",
    "# torch.FloatTensor([2.,3.2]) 少使用,易混淆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "# set default type\n",
    "print(torch.tensor([1.2,3]).type())\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(torch.tensor([1.2,3]).type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note5: tensor和FloatTensor\n",
    "- 小写给具体数据,大写给shape\n",
    "- 这种方法生成的张量不能直接使用,里面会存在无穷大和无穷小,后面必须用别的数据写入\n",
    "- .set_default_tensor_type()方法可以设置tensor的默认类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2782, 0.6091, 0.6666],\n",
      "        [0.3973, 0.5082, 0.8371],\n",
      "        [0.4828, 0.8271, 0.6257]])\n",
      "tensor([[0.2782, 0.6091, 0.6666],\n",
      "        [0.3973, 0.5082, 0.8371],\n",
      "        [0.4828, 0.8271, 0.6257]])\n",
      "tensor([[5, 4, 8],\n",
      "        [3, 3, 2],\n",
      "        [6, 8, 2]])\n"
     ]
    }
   ],
   "source": [
    "# rand\n",
    "a = torch.rand(3,3)\n",
    "print(a)\n",
    "b = torch.rand_like(a)\n",
    "print(a)\n",
    "\n",
    "a = torch.randint(1,10,[3,3])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note6: rand/rand_like , randint\n",
    "- rand会均匀随机产生[0,1)之间的数据\n",
    "- *_like方法接收的参数是tensor\n",
    "- .randint(1,10,[3,3])产生[1,10)之间的均匀分布随机数,shape为[3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7122, 0.9045, 0.0039],\n",
      "        [0.8279, 0.5099, 0.2207],\n",
      "        [0.0466, 0.6949, 0.9866]])\n",
      "tensor([ 1.8412,  0.0322,  0.7769,  0.7791,  0.2126,  0.1222, -0.2154,  0.2899,\n",
      "         0.3680,  0.1192])\n",
      "\n",
      "\n",
      "\n",
      "想把tensor全部赋值为一个元素用full: \n",
      " tensor([[7., 7., 7.],\n",
      "        [7., 7., 7.]])\n",
      "生成一个标量:\n",
      " tensor(7.)\n",
      "生成一个vec:\n",
      " tensor([7., 7.])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# randn正太分布   N(0,1)\n",
    "a = torch.rand(3,3)\n",
    "print(a)\n",
    "\n",
    "# N(u,std)\n",
    "print(torch.normal(mean=torch.full([10],0),std=torch.arange(1,0,-0.1)))\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# full\n",
    "print('想把tensor全部赋值为一个元素用full: \\n',torch.full([2,3],7))\n",
    "print('生成一个标量:\\n',torch.full([],7))\n",
    "print('生成一个vec:\\n',torch.full([2],7))\n",
    "\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note7: randn , full\n",
    "- randn正态分布\n",
    "- full将tensor全部赋值为一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arange生成[0,10)内的等差为2的数列: \n",
      " tensor([0, 2, 4, 6, 8])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# arrange/range\n",
    "print('arange生成[0,10)内的等差为2的数列: \\n',torch.arange(0,10,2))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linespace在[0,10]中生成4个间隔相同的数字: \n",
      " tensor([ 0.0000,  3.3333,  6.6667, 10.0000])\n",
      "logspace在[-1,0]中切割10份,设每个数为xi,生成10^xi: \n",
      " tensor([1.0000, 0.7743, 0.5995, 0.4642, 0.3594, 0.2783, 0.2154, 0.1668, 0.1292,\n",
      "        0.1000])\n"
     ]
    }
   ],
   "source": [
    "# linespace/logspace\n",
    "print('linespace在[0,10]中生成4个间隔相同的数字: \\n',torch.linspace(0,10,steps=4))\n",
    "print('logspace在[-1,0]中切割10份,设每个数为xi,生成10^xi: \\n',torch.logspace(0,-1,steps=10,base=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note8: arange/range  linespace/logspace\n",
    "- pytorch不建议使用range\n",
    "- arange的第三个参数是等差\n",
    "- linespace的第三个参数是等分的数量\n",
    "- logspace的base参数可以设置为2,10,e等底数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones(3,3))\n",
    "print(torch.zeros(3,3))\n",
    "print(torch.eye(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note9: ones/zeros/eye , ones_like/zeros_like/eye_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成[0,10)的随机索引, 可以用来shuffle : \n",
      " tensor([4, 1, 9, 3, 2, 6, 8, 7, 0, 5])\n"
     ]
    }
   ],
   "source": [
    "print('生成[0,10)的随机索引, 可以用来shuffle : \\n',torch.randperm(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note10: randperm \n",
    "- 生成随机索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 索引与切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a的shape:  torch.Size([4, 3, 28, 28])\n",
      "a[0]的shape:   torch.Size([3, 28, 28])\n",
      "a[0,0]的shape:   torch.Size([28, 28])\n",
      "a[0,0,1,1]的shape:   torch.Size([]) \n",
      "\n",
      "选取连续范围的张量:\n",
      "a的shape:  torch.Size([4, 3, 28, 28])\n",
      "a[:2]的shape:  torch.Size([2, 3, 28, 28])\n",
      "a[:2,:1,:,:]:  torch.Size([2, 1, 28, 28])\n",
      "a[:2,1:,:,:]:  torch.Size([2, 2, 28, 28])\n",
      "a[:2,-1:,:,:]:  torch.Size([2, 1, 28, 28]) \n",
      "\n",
      "选取带step的张量\n",
      "a的shape:  torch.Size([4, 3, 28, 28])\n",
      "a[:,:,0:28:2,0:28:2]的shape:  torch.Size([4, 3, 14, 14])\n",
      "a[:,:,::2,::2].shape:  torch.Size([4, 3, 14, 14]) \n",
      "\n",
      "根据具体的索引采样\n",
      "a的shape:  torch.Size([4, 3, 28, 28])\n",
      "a.index_select(0,torch.tensor([0,2])).shape 第0维度上索引为[0,2]的两张图片 : torch.Size([2, 3, 28, 28])\n",
      "a.index_select(2,torch.arange(8)).shape  第2个维度上索引为0-8行的内容:  torch.Size([4, 3, 8, 28]) \n",
      "\n",
      "用...来索引, ...代表任意多的维度\n",
      "a的shape:  torch.Size([4, 3, 28, 28])\n",
      "a[...].shape:  torch.Size([4, 3, 28, 28])\n",
      "a[:,1,...].shape  torch.Size([4, 28, 28]) \n",
      "\n",
      "使用掩码索引\n",
      "x.shape:  torch.Size([3, 4])\n",
      "torch.masked_select(x,mask):  tensor([0.9662, 1.1723, 0.7542])\n",
      "Shape为:  torch.Size([3])\n",
      "\n",
      "用take索引,即先flatten后再索引,\n",
      "tensor([[4, 3, 5],\n",
      "        [6, 7, 8]]) \n",
      " torch.Size([2, 3])\n",
      "torch.take(a,torch.tensor([0,2,5])):  tensor([4, 5, 8])\n",
      "Shape:  torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4,3,28,28) # [batch_size , channel , Height, width]\n",
    "print('a的shape: ',a.shape)\n",
    "print('a[0]的shape:  ',a[0].shape)\n",
    "print('a[0,0]的shape:  ',a[0,0].shape)\n",
    "print('a[0,0,1,1]的shape:  ',a[0,0,1,1].shape,'\\n') # 标量\n",
    "\n",
    "print('选取连续范围的张量:')\n",
    "print('a的shape: ',a.shape)\n",
    "print('a[:2]的shape: ',a[:2].shape)\n",
    "print('a[:2,:1,:,:]: ',a[:2,:1,:,:].shape) # 单个:表示取全部\n",
    "print('a[:2,1:,:,:]: ',a[:2,1:].shape)\n",
    "print('a[:2,-1:,:,:]: ',a[:2,-1:,:,:].shape,'\\n') # 负数代表反向取\n",
    "\n",
    "print('选取带step的张量')\n",
    "print('a的shape: ',a.shape)\n",
    "print('a[:,:,0:28:2,0:28:2]的shape: ',a[:,:,0:28:2,:28:2].shape)\n",
    "print('a[:,:,::2,::2].shape: ',a[:,:,::2,::2].shape,'\\n')\n",
    "\n",
    "print('根据具体的索引采样')\n",
    "print('a的shape: ',a.shape)\n",
    "print('a.index_select(0,torch.tensor([0,2])).shape 第0维度上索引为[0,2]的两张图片 :',\n",
    "      a.index_select(0,torch.tensor([0,2])).shape)\n",
    "print('a.index_select(2,torch.arange(8)).shape  第2个维度上索引为0-8行的内容: ',\n",
    "     a.index_select(2,torch.arange(8)).shape,'\\n')\n",
    "\n",
    "print('用...来索引, ...代表任意多的维度')\n",
    "print('a的shape: ',a.shape)\n",
    "print('a[...].shape: ',a[...].shape)\n",
    "print('a[:,1,...].shape ',a[:,1,...].shape,'\\n')\n",
    "\n",
    "\n",
    "print('使用掩码索引')\n",
    "x = torch.randn(3,4)\n",
    "print('x.shape: ',x.shape)\n",
    "mask = x.ge(0.5) # GreaterEqual,大于等于0.5 ,mask中大于等于5置为1,其余为0\n",
    "print('torch.masked_select(x,mask): ',torch.masked_select(x,mask))\n",
    "print('Shape为: ',torch.masked_select(x,mask).shape)\n",
    "\n",
    "print('\\n用take索引,即先flatten后再索引,')\n",
    "a = torch.tensor([[4,3,5],[6,7,8]])\n",
    "print(a,'\\n',a.shape)\n",
    "print('torch.take(a,torch.tensor([0,2,5])): ',torch.take(a,torch.tensor([0,2,5])))\n",
    "print('Shape: ',torch.take(a,torch.tensor([0,2,5])).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note11: 索引与切片的多种方式\n",
    "- 选取连续范围的张量\n",
    "- 选取带step的张量\n",
    "- 根据具体的索引采样\n",
    "- 用...来索引, ...代表任意多的维度\n",
    "- 使用掩码索引,结果是个一维向量\n",
    "- 使用take索引,先flatten打平然后索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 维度变换\n",
    "- View/reshape\n",
    "- Squeeze/unsqueeze\n",
    "- Transpose/t/permute\n",
    "- Expand/repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view/reshape:\n",
      "a.shape:  torch.Size([4, 1, 28, 28])\n",
      "a.view(4,28*28):  torch.Size([4, 784]) \n",
      "\n",
      "Squeeze v.s. unsqueeze:\n",
      "a.shape:  torch.Size([4, 1, 28, 28])\n",
      "参数是正数,在这个索引之前插入,参数是负数,在这个索引之后插入\n",
      "a.unsqueeze(0):  torch.Size([1, 4, 1, 28, 28])\n",
      "a.unsqueeze(-1):  torch.Size([4, 1, 28, 28, 1])\n",
      "a:  tensor([1.2000, 2.3000])\n",
      "a.unsqueeze(-1):  tensor([[1.2000],\n",
      "        [2.3000]])   torch.Size([2, 1])\n",
      "a.unsqueeze(0):  tensor([[1.2000, 2.3000]])   torch.Size([1, 2])\n",
      "\n",
      "unsqueeze实例:\n",
      "unsqueeze b 之前:  torch.Size([32])\n",
      "b.unsqueeze(1).unsqueeze(2).unsqueeze(0)后:  torch.Size([1, 32, 1, 1])\n",
      "\n",
      "squeeze操作:\n",
      "b.shape:  torch.Size([1, 32, 1, 1])\n",
      "b.squeeze()后:  torch.Size([32])\n",
      "b.squeeze(0)后:  torch.Size([32, 1, 1])\n",
      "b.squeeze(-4)后:  torch.Size([32, 1, 1])\n",
      "\n",
      "expand操作:\n",
      "b.shape:  torch.Size([1, 32, 1, 1])\n",
      "b.expand(4,32,14,14)后:  torch.Size([4, 32, 14, 14])\n",
      "b.expand(-1,32,-1,-1)后:  torch.Size([1, 32, 1, 1])\n",
      "\n",
      "repeat操作:\n",
      "b.shape:  torch.Size([1, 32, 1, 1])\n",
      "b.repeat(4,32,1,1)后:  torch.Size([4, 1024, 1, 1])\n",
      "repeat中的参数表示重复的次数,所以是32*32=1024\n"
     ]
    }
   ],
   "source": [
    "# view reshape等价\n",
    "a = torch.rand(4,1,28,28)\n",
    "print('view/reshape:')\n",
    "print('a.shape: ',a.shape)\n",
    "print('a.view(4,28*28): ',a.view(4,28*28).shape,'\\n')\n",
    "\n",
    "print('Squeeze v.s. unsqueeze:')\n",
    "print('a.shape: ',a.shape)\n",
    "print('参数是正数,在这个索引之前插入,参数是负数,在这个索引之后插入')\n",
    "print('a.unsqueeze(0): ',a.unsqueeze(0).shape)\n",
    "print('a.unsqueeze(-1): ',a.unsqueeze(-1).shape)\n",
    "a = torch.tensor([1.2,2.3])\n",
    "print('a: ',a)\n",
    "print('a.unsqueeze(-1): ',a.unsqueeze(-1),' ',a.unsqueeze(-1).shape)\n",
    "print('a.unsqueeze(0): ',a.unsqueeze(0),' ',a.unsqueeze(0).shape)\n",
    "\n",
    "print('\\nunsqueeze实例:')\n",
    "b = torch.rand(32)\n",
    "f = torch.rand(4,32,14,14)\n",
    "print('unsqueeze b 之前: ',b.shape)\n",
    "b = b.unsqueeze(1).unsqueeze(2).unsqueeze(0)\n",
    "\n",
    "print('b.unsqueeze(1).unsqueeze(2).unsqueeze(0)后: ',b.shape)\n",
    "\n",
    "print('\\nsqueeze操作:')\n",
    "print('b.shape: ',b.shape)\n",
    "print('b.squeeze()后: ',b.squeeze().shape)\n",
    "print('b.squeeze(0)后: ',b.squeeze(0).shape)\n",
    "print('b.squeeze(-4)后: ',b.squeeze(-4).shape)\n",
    "\n",
    "print('\\nexpand操作:')\n",
    "a = torch.rand(4,32,14,14)\n",
    "print('b.shape: ',b.shape)\n",
    "print('b.expand(4,32,14,14)后: ',b.expand(4,32,14,14).shape)\n",
    "print('b.expand(-1,32,-1,-1)后: ',b.expand(-1,32,-1,-1).shape)\n",
    "\n",
    "print('\\nrepeat操作:')\n",
    "print('b.shape: ',b.shape)\n",
    "print('b.repeat(4,32,1,1)后: ',b.repeat(4,32,1,1).shape)\n",
    "print('repeat中的参数表示重复的次数,所以是32*32=1024')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note12: 维度变换\n",
    "- view/reshape\n",
    "- squeeze v.s. unsqueeze\n",
    "- 维度扩张\n",
    " - expand:broadcasting(推荐使用)\n",
    " - repeat:memory copied (参数是重复的次数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[0.8286, 0.4806, 0.9881, 0.5412],\n",
      "        [0.5168, 0.8315, 0.1288, 0.5200],\n",
      "        [0.5800, 0.0461, 0.9936, 0.5407]])\n",
      "a.T:\n",
      " tensor([[0.8286, 0.5168, 0.5800],\n",
      "        [0.4806, 0.8315, 0.0461],\n",
      "        [0.9881, 0.1288, 0.9936],\n",
      "        [0.5412, 0.5200, 0.5407]])\n",
      ".T操作只适用于2D tensor\n",
      "\n",
      "a.shape:  torch.Size([4, 3, 32, 32])\n",
      "a.transpose(1,3).contiguous().view(4,3*32*32).view(4,3,32,32):\n",
      " torch.Size([4, 3, 32, 32]) \n",
      "\n",
      "a.transpose(1,3).contiguous(),view(4,3*32*32).view(4,32,32,3).transpose(1,3):\n",
      " torch.Size([4, 3, 32, 32]) \n",
      "\n",
      "a1.shape:  torch.Size([4, 3, 32, 32])\n",
      "a2.shape:  torch.Size([4, 3, 32, 32])\n",
      "a = a1 ?  tensor(False)\n",
      "a = a2 ?  tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# 转秩操作\n",
    "a = torch.rand(3,4)\n",
    "print('a:\\n',a)\n",
    "print('a.T:\\n',a.T)\n",
    "print('.T操作只适用于2D tensor\\n')\n",
    "\n",
    "a = torch.rand(4,3,32,32)\n",
    "print('a.shape: ',a.shape)\n",
    "a1 = a.transpose(1,3).contiguous().view(4,3*32*32).view(4,3,32,32)\n",
    "print('a.transpose(1,3).contiguous().view(4,3*32*32).view(4,3,32,32):\\n',a1.shape,'\\n')\n",
    "a2 = a.transpose(1,3).contiguous().view(4,3*32*32).view(4,32,32,3).transpose(1,3)\n",
    "print('a.transpose(1,3).contiguous(),view(4,3*32*32).view(4,32,32,3).transpose(1,3):\\n',a2.shape,'\\n')\n",
    "\n",
    "print('a1.shape: ',a1.shape)\n",
    "print('a2.shape: ',a2.shape)\n",
    "print('a = a1 ? ',torch.all(torch.eq(a,a1)))\n",
    "print('a = a2 ? ',torch.all(torch.eq(a,a2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note12: Transpose\n",
    "a.shape # [4,3,32,32] -> [b c h w] \n",
    "\n",
    "如果要交换第一维和第三维 , a.transpose(1,3)  : [b c h w] -> [b w h c] , \n",
    "\n",
    "如果把后三个维度连在一起理解, 即 a.transpose(1,3).view(4,3*32*32) , \n",
    "\n",
    "再展开a.transpose(1,3).view(4,3*32*32).view(4,3,32,32) , -> [b c w h]\n",
    "\n",
    "由于view操作会把维度信息丢掉,使用transpose后,[b c h w] -> [b w h c], 后面进行view(4,3,32,32)操作时候,是按[b c h w]的顺序展开的,这是错误的,因为维度顺序已经变成了[b w h c ],正确的写法是view(4,32,32,3), **数据的维度顺序必须和存储顺序一致** \n",
    "\n",
    "transpose涉及维度的交换,原来的数据存储方式会发生改变,数据会变得不连续,需要使用.contiguous()方法把数据重新变成连续的,然后再view\n",
    "\n",
    "- 用.eq比较a和a1是否相等,再用torch.all()方法表示所有元素都是一致的时候才返回True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用transpose方法:\n",
      "a.shape:  torch.Size([4, 3, 28, 32])\n",
      "a.transpose(1,3).transpose(1,2):  torch.Size([4, 28, 32, 3])\n",
      "等价于:\n",
      "a.permute(0,2,3,1):  torch.Size([4, 28, 32, 3])\n"
     ]
    }
   ],
   "source": [
    "# permute\n",
    "a = torch.rand(4,3,28,32) # 变换为 -> [4,28,32,3]\n",
    "print('用transpose方法:')\n",
    "print('a.shape: ',a.shape)\n",
    "print('a.transpose(1,3).transpose(1,2): ',a.transpose(1,3).transpose(1,2).shape)\n",
    "print('等价于:')\n",
    "print('a.permute(0,2,3,1): ',a.permute(0,2,3,1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note13: permute\n",
    "- 可以完成任意次数的交换,按照交换前后的索引号填写参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python35664bitbaseconda34414b764a4544e4b3502fc9f239efc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
