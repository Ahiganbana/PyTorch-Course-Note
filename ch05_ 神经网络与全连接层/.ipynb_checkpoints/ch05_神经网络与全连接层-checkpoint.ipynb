{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch05_神经网络与全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Logistic ( sigmoid ) Regression \n",
    "\n",
    "- for continuous (由于是continuous,所以才叫做regression):\n",
    "$$ y = x w + b$$\n",
    "- for probability output :  \n",
    "$$ y = \\sigma ( x w + b )$$\n",
    " - $\\sigma$: sigmoid or logistic\n",
    "\n",
    "### Binary Classification :\n",
    "- interpred network as $ f : x \\to p( y \\mid x ; \\theta) $\n",
    "- output $ \\in [ 0 , 1 ]$\n",
    "- which is exactly what logistic function comes in\n",
    "\n",
    "### For Regression:\n",
    "- Goal : $ pred = y $\n",
    "- Approach : minimize $ dist(pred,y) $\n",
    "\n",
    "### For classification:\n",
    "- Goal : maximize benchmark , e.g. accuracy\n",
    "- Approach 1:  minimize $ dist ( p_\\theta ( y \\mid x ) , p_r ( y \\mid x )) $\n",
    "- Approach 2:  minimize $ divergence ( p_\\theta ( y \\mid x ) , p_r (y \\mid x )) $\n",
    "\n",
    "### Why call logistic regression ?\n",
    "- use sigmoid\n",
    "- Controversial:\n",
    " - MSE $\\to$ regression\n",
    " - Cross Entropy $\\to$ classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Cross Entropy Loss ( 交叉熵 )\n",
    "\n",
    "### Loss for classification:\n",
    "- MSE\n",
    "- Hinge Loss (SVM)\n",
    "$$ \\sum_i \\max (0 , 1-y_i * h_\\theta(x_i)) $$\n",
    "- Cross Entropy Loss\n",
    "\n",
    "### What's Entropy means ? \n",
    "- Uncertainty ( 不确定性 )\n",
    " - measure of surprise ( 惊喜度 )\n",
    "- higher entropy = less info.\n",
    "$$ Entropy = - \\sum_i P(i)\\space \\log P(i) $$\n",
    "\n",
    "### Binary Classification\n",
    "$$ H(P,Q) = -P(cat)\\log Q(cat) - (1-P(cat))\\log (1-Q(cat))$$\n",
    "$$由于是二分类,所以 P(dog) = (1-P(cat))$$\n",
    "$$\n",
    "H(P,Q) = - \\sum_{i=(cat,dog)}P(i)\\log Q(i) \\\\\n",
    "= -P(cat)\\log Q(cat) - P(dog)\\log Q(dog)  \\\\\n",
    " -(y\\log (p) + (1-y)\\log (1-p))\n",
    "$$\n",
    "\n",
    "### Why not use MSE on classification\n",
    "- sigmoid + MSE $\\to$ gradient vanish\n",
    "- converge slower\n",
    "- But,sometimes\n",
    " - e.g. meta-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([1, 784]) \n",
      "w.shape:  torch.Size([10, 784]) \n",
      "logits = x@w.t(),logits.shape:  torch.Size([1, 10])\n",
      "pred = softmax(logits,dim=1) :\n",
      " tensor([[2.8702e-04, 4.0199e-01, 1.3270e-05, 9.6294e-04, 1.8527e-02, 2.3045e-01,\n",
      "         7.3673e-03, 4.2058e-04, 3.3930e-01, 6.7891e-04]])\n",
      "log(pred): tensor([[ -8.1559,  -0.9113, -11.2300,  -6.9455,  -3.9885,  -1.4677,  -4.9107,\n",
      "          -7.7739,  -1.0809,  -7.2950]])\n",
      "方法一:使用F.cross_entropy(logits,torch.tensor([3])直接计算: tensor(6.9455)\n",
      "方法二:使用softmax计算出的pred_log计算: tensor(6.9455)\n",
      "方法一只需要一步,而方法二的CE = softmax -> logits -> nll_loss\n"
     ]
    }
   ],
   "source": [
    "# Numerical Stability\n",
    "x = torch.rand(1,784)\n",
    "w = torch.rand(10,784)\n",
    "logits = x@w.t()\n",
    "print('x.shape: ',x.shape,'\\nw.shape: ',w.shape,\n",
    "      '\\nlogits = x@w.t(),logits.shape: ',logits.shape)\n",
    "\n",
    "pred = torch.softmax(logits,dim=1)\n",
    "print('pred = softmax(logits,dim=1) :\\n',pred)\n",
    "pred_log = torch.log(pred)\n",
    "print('log(pred):',pred_log)\n",
    "\n",
    "loss = F.cross_entropy(logits,torch.tensor([3]))\n",
    "print('方法一:使用F.cross_entropy(logits,torch.tensor([3])直接计算:',loss)\n",
    "my_loss = F.nll_loss(pred_log,torch.tensor([3]))\n",
    "print('方法二:使用softmax计算出的pred_log计算:',my_loss)\n",
    "print('方法一只需要一步,而方法二的CE = softmax -> logits -> nll_loss')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 多分类问题实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 3.083690\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.528396\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.577129\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.460878\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.586255\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.624034\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.334806\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.431546\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.629640\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.407158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/distiller/project/conda/conda-bld/pytorch_1591914879026/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0071, Accuracy: 8427/10000 (84%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.619346\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.550431\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.452561\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.439310\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.529607\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.257746\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.293476\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.316650\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.297505\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.440608\n",
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 8538/10000 (85%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.480287\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.530437\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.350130\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.409370\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.333595\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.396966\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.434597\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.492732\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.391622\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.388209\n",
      "\n",
      "Test set: Average loss: 0.0060, Accuracy: 8597/10000 (86%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.437538\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.258717\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.403510\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.407749\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.234885\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.279934\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.370400\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.444407\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.498015\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.557527\n",
      "\n",
      "Test set: Average loss: 0.0056, Accuracy: 8659/10000 (87%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.316698\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.409586\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.511134\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.567208\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.456032\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.232997\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.335775\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.349151\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.449998\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.337424\n",
      "\n",
      "Test set: Average loss: 0.0054, Accuracy: 8708/10000 (87%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.262621\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.442502\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.381229\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.424382\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.339480\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.156607\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.543846\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.370581\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.180045\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.728054\n",
      "\n",
      "Test set: Average loss: 0.0053, Accuracy: 8725/10000 (87%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.278354\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.254100\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.430109\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.319065\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.408925\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.181885\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.255681\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.395882\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.179406\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.424512\n",
      "\n",
      "Test set: Average loss: 0.0052, Accuracy: 8755/10000 (88%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.178965\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.530159\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.092848\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.296255\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.399122\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.278785\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.234787\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.338992\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.452053\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.372501\n",
      "\n",
      "Test set: Average loss: 0.0051, Accuracy: 8770/10000 (88%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.293521\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.329178\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.304065\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.279119\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.398582\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.370215\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.211270\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.380234\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.280856\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "\n",
    "learning_rate = 1e-2\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "train_load = torch.utils.data.DataLoader(torchvision.datasets.MNIST(\n",
    "    '../data/',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "    ])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "test_load = torch.utils.data.DataLoader(torchvision.datasets.MNIST(\n",
    "    '../data/',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307),(0.3081,))])),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "\n",
    "\n",
    "w1,b1 = torch.randn(200,784,requires_grad=True),\\\n",
    "        torch.zeros(200,requires_grad=True)\n",
    "w2,b2 = torch.randn(200,200,requires_grad=True),\\\n",
    "        torch.zeros(200,requires_grad=True)\n",
    "w3,b3 = torch.randn(10,200,requires_grad=True),\\\n",
    "        torch.zeros(10,requires_grad=True)\n",
    "\n",
    "nn.init.kaiming_normal_(w1)\n",
    "nn.init.kaiming_normal_(w2)\n",
    "nn.init.kaiming_normal_(w3)\n",
    "\n",
    "def forward(x):\n",
    "    x = x@w1.t() + b1\n",
    "    x = F.relu(x)\n",
    "    x = x@w2.t() + b2\n",
    "    x = F.relu(x)\n",
    "    x = x@w3.t() + b3\n",
    "    x = F.relu(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "optimizer = optim.SGD([w1,b1,w2,b2,w3,b3],lr=learning_rate)\n",
    "criteon = nn.CrossEntropyLoss()\n",
    "global_step = 0\n",
    "global_test_step = 0\n",
    "# criteon = F.cross_entropy()\n",
    "\n",
    "global_step = 0\n",
    "vis = Visdom()\n",
    "vis.line([0.],[0.],win='train_loss',opts=dict(title='train_loss'))\n",
    "vis.line([[0.0,0.0]],[0.],win='test',opts=dict(title='test loss&acc.',legend=['loss','acc.']))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data,target) in enumerate(train_load):\n",
    "        data = data.view(-1,28*28)\n",
    "        logits = forward(data)\n",
    "        loss = criteon(logits,target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx%100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.\n",
    "                 format(epoch,batch_idx*len(data),len(train_load.dataset),\n",
    "                       100. *batch_idx/len(train_load),loss.item()))\n",
    "        global_step += 1\n",
    "        vis.line([loss.item()],[global_step],win='train_loss',update='append')\n",
    "            \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data,target in test_load:\n",
    "        \n",
    "        vis.images(data.view(-1,1,28,28),win='x')\n",
    "        \n",
    "        data = data.view(-1,28*28)\n",
    "        logits = forward(data)\n",
    "        test_loss += criteon(logits,target).item()\n",
    "        pred = logits.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).sum()\n",
    "        \n",
    "        global_test_step += 1\n",
    "        vis.line([[test_loss,correct/len(test_load.dataset)]],[global_test_step],win='test',update='append')\n",
    "    \n",
    "       \n",
    "        vis.text(str(pred.detach().cpu().numpy()),win='pred',opts=dict(title='pred'))\n",
    "    \n",
    "    test_loss /= len(test_load.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.\n",
    "         format(test_loss,correct,len(test_load.dataset),100. *correct/len(test_load.dataset)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 全连接层\n",
    "\n",
    "- x = F.relu(x,inplace = True)\n",
    " - inplace = True 代表进行原地操作,可以节省一半空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Visdom 可视化\n",
    "- 安装:\n",
    " - python -m pip install --upgrade pip\n",
    " - python -m pip install visdom\n",
    "- 启动:\n",
    " - python -m visdom.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python35664bitbaseconda34414b764a4544e4b3502fc9f239efc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
