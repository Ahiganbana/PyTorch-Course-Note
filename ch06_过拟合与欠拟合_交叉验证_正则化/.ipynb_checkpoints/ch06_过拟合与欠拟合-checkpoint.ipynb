{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 过拟合与欠拟合\n",
    "\n",
    "- **Underfitted 欠拟合**\n",
    " - train acc. is bad\n",
    " - test acc. is bad as well\n",
    " \n",
    "- **Overfitted 过拟合**\n",
    " - train loss and acc. is much better\n",
    " - test acc. is worse\n",
    " - ->Generalization Performance 泛化能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting -> Train Set & Test Set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms , datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data/',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "                   ])),\n",
    "                   \n",
    "                   shuffle=True)\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data/',\n",
    "                   train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307, ), (0.3081, ))])),\n",
    "                   shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting -> Train Set & Val Set & Test Set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_:  60000 test:  10000\n",
      "train: 50000   val: 10000\n",
      "train: 782   val: 157\n"
     ]
    }
   ],
   "source": [
    "print('train_: ',len(train_loader),'test: ',len(test_loader))\n",
    "train_db,val_db = torch.utils.data.random_split(train_loader,[50000,10000])\n",
    "print('train:',len(train_db),'  val:',len(val_db))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_db,batch_size=64,shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_db,batch_size=64,shuffle=True)\n",
    "\n",
    "print('train:',len(train_loader),'  val:',len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross-validation :\n",
    "\n",
    "- merge train/val sets\n",
    "- randomly sample 1/k as val set\n",
    "\n",
    "\n",
    "如果数据区间为[0,60k],根据上述划分:\n",
    "- [0,50k]为train set,\n",
    "- [50k,60k]为val set,\n",
    "- [60k,70k]为test set\n",
    "\n",
    "上面这个方法,val-set的数据没有参与训练的反向传播,\n",
    "而 K-fold Cross-Validation的划分为:\n",
    "- epoch 1:\n",
    " - [0,50k] -> train set,\n",
    " - [50k,60k] -> val set,\n",
    " - [60k,70k] -> test set\n",
    "\n",
    "- epoch 2:\n",
    " - [0,40k] [50k,60k] -> train set,\n",
    " - [40k,50k] -> val set,\n",
    " - [60k,70k] -> test set\n",
    " \n",
    "- epoch 3:\n",
    " - [0,30k] [40k,60k] -> train set,\n",
    " - [30k,40k] -> val set,\n",
    " - [60k,70k] -> test set\n",
    " \n",
    "- ....\n",
    "\n",
    "具体实践中的划分视情况而定,上面的例子主要是为了表达 K-fold Cross-Validation 的思想\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
